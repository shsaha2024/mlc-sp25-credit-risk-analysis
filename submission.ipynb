{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlc.cashflow import ScorableModelTemplate, compute_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import xgboost as xgb\n",
    "import os\n",
    "\n",
    "# Standard library\n",
    "import warnings\n",
    "import ast\n",
    "from pathlib import Path\n",
    "from datetime import timedelta\n",
    "from abc import ABC, abstractmethod\n",
    "from collections import Counter\n",
    "\n",
    "# Third-party libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import ttest_ind, skew, mannwhitneyu, kurtosis, entropy\n",
    "from joblib import Parallel, delayed\n",
    "import xgboost as xgb\n",
    "\n",
    "# scikit-learn\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    StratifiedKFold,\n",
    "    RepeatedStratifiedKFold,\n",
    "    GridSearchCV,\n",
    "    cross_val_score\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    "    make_scorer\n",
    ")\n",
    "\n",
    "from Final_workflow.c04_script import model_4\n",
    "from Final_workflow.c02_script import model_2\n",
    "from Final_workflow.c03 import features_c03, c03_predictions\n",
    "from Final_workflow.c01_script import model_1\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "import sys\n",
    "init_dir = os.getcwd()\n",
    "\n",
    "class ScorableModel(ScorableModelTemplate):\n",
    "    def predict(self, raw_consumer_file: str, raw_transactions_file: str):\n",
    "        \"\"\"Predict labels and positions of bugs.\n",
    "\n",
    "        :param raw_consumer_file: path to consumer_data.parquet\n",
    "        :param raw_consumer_file: path to transactions.parquet\n",
    "        \"\"\" \n",
    "        loader = self.process_inputs(raw_consumer_file, raw_transactions_file)\n",
    "        consumer_df = pd.read_parquet(raw_consumer_file)\n",
    "        placeholder = consumer_df[['masked_consumer_id']]\n",
    "        predictions = []\n",
    "\n",
    "        for i in range(1,5):\n",
    "            if i == 4 and len(consumer_df[consumer_df['masked_consumer_id'].str[2] == str(4)]) != 0:\n",
    "                c04_results = consumer_df[consumer_df['masked_consumer_id'].str[2] == str(4)]\n",
    "                transactions_4 = loader[loader['masked_consumer_id'].str[2] == '4']\n",
    "                pred = model_4(transactions_4)\n",
    "                pred_df = pd.DataFrame({'masked_consumer_id': pred.index, 'y_pred': pred.values}).reset_index(drop=True)\n",
    "                merged = c04_results.merge(pred_df, on='masked_consumer_id', how='left').fillna(0.5)\n",
    "                predictions.append(merged[['masked_consumer_id', 'y_pred']])\n",
    "            if i == 3 and len(consumer_df[consumer_df['masked_consumer_id'].str[2] == str(3   )]) != 0:\n",
    "                features = features_c03(loader, raw_consumer_file)\n",
    "                c03_results, features = features[['masked_consumer_id']], features.drop(columns = ['masked_consumer_id'])\n",
    "                c03_results['y_pred'] = c03_predictions(features)\n",
    "                predictions.append(c03_results)\n",
    "            if i == 2 and len(consumer_df[consumer_df['masked_consumer_id'].str[2] == str(2)]) != 0:\n",
    "                c02_results = consumer_df[consumer_df['masked_consumer_id'].str[2] == str(2)]\n",
    "                transactions_2 = loader[loader['masked_consumer_id'].str[2] == '2']\n",
    "                pred = model_2(transactions_2, c02_results)\n",
    "                pred_df = pd.DataFrame({'masked_consumer_id': pred.index, 'y_pred': pred.values}).reset_index(drop=True)\n",
    "                merged = c02_results.merge(pred_df, on='masked_consumer_id', how='left').fillna(0.5)\n",
    "                predictions.append(merged[['masked_consumer_id', 'y_pred']])\n",
    "            if i == 1 and len(consumer_df[consumer_df['masked_consumer_id'].str[2] == str(1)]) != 0:\n",
    "                c01_results = consumer_df[consumer_df['masked_consumer_id'].str[2] == str(1)]\n",
    "                transactions_1 = loader[loader['masked_consumer_id'].str[2] == '1']\n",
    "                pred = model_1(transactions_1, c01_results)\n",
    "                pred_df = pd.DataFrame({'masked_consumer_id': pred.index, 'y_pred': pred.values}).reset_index(drop=True)\n",
    "                merged = c01_results.merge(pred_df, on='masked_consumer_id', how='left').fillna(0.5)\n",
    "                predictions.append(merged[['masked_consumer_id', 'y_pred']])\n",
    "\n",
    "        all_pred = pd.concat(predictions)\n",
    "        end_result = placeholder.merge(all_pred, on = 'masked_consumer_id', how='left')\n",
    "        pred = end_result['y_pred'].values\n",
    "\n",
    "        return pred\n",
    "\n",
    "    def process_inputs(self, raw_consumer_file: str, raw_transactions_file: str):\n",
    "        \"\"\"Input argument will vary. See you competition's template.\n",
    "\n",
    "        :param raw_files: list of file path strings, depends on competition\n",
    "        :return: anything needed for you model to make predictions, e.g. features or processed data\n",
    "        \"\"\"\n",
    "        # Read in files\n",
    "        df_consumer = pd.read_parquet(raw_consumer_file)\n",
    "        df_transactions = pd.read_parquet(raw_transactions_file)\n",
    "\n",
    "        # Merge and filter to get only valid data\n",
    "        merged_df = df_consumer.merge(df_transactions, on = \"masked_consumer_id\", how = 'left')\n",
    "        filtered_df = merged_df[merged_df[\"posted_date\"]< merged_df[\"evaluation_date\"]]\n",
    "\n",
    "        # Clean evaluation_date and create evaluation_day for day of week of evaluation_date\n",
    "        filtered_df['evaluation_date'] = pd.to_datetime(filtered_df['evaluation_date'])\n",
    "        filtered_df['evaluation_day'] = filtered_df['evaluation_date'].dt.dayofweek\n",
    "\n",
    "\n",
    "\n",
    "        # Clean posted_date and create posted_day for day of week of posted_date\n",
    "        filtered_df['posted_date'] = pd.to_datetime(filtered_df['posted_date'])\n",
    "        filtered_df['posted_day'] = filtered_df['posted_date'].dt.dayofweek\n",
    "\n",
    "        # Get loan category from masked_consumer_id\n",
    "        filtered_df['loan_category'] = filtered_df['masked_consumer_id'].str[2].astype(int)\n",
    "        filtered_df['converted_date'] = (max(filtered_df['posted_date']) - filtered_df['posted_date']).dt.total_seconds()/3600\n",
    "        \n",
    "        return filtered_df\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "# Intialize, runs: __check_rep__ to validate class\n",
    "model = ScorableModel() # error will be raised if the above is not implemented correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, true_output = model.load_test_case()\n",
    "predicted_output = model.predict(\"mlc/test_data/consumer_data.parquet\", \"mlc/test_data/transactions.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "score = compute_score(true_output, predicted_output)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_score_2(df_consumer: pd.DataFrame, y_pred: np.ndarray):\n",
    "\n",
    "    df_consumer['group_id'] = df_consumer['masked_consumer_id'].str[:3]\n",
    "    df_consumer['y_pred'] = y_pred\n",
    "    return [roc_auc_score(df[\"FPF_TARGET\"], df[\"y_pred\"]) for _, df in df_consumer.groupby('group_id')]\n",
    "\n",
    "compute_score_2(true_output, predicted_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
